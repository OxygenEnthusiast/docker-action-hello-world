{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1Hm87s_N3gDAd1bLaOcywZBGRWVzE1Wn9",
      "authorship_tag": "ABX9TyMgdNR1BkuQqznRS6ro0L3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OxygenEnthusiast/docker-action-hello-world/blob/main/gen_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In your google drive, make sure to add the corresponding project directory containing the subdirectory \"images\" found in the [omi-repo](https://github.com/jcpeterson/omi) as well as lhe `attribute_means.csv` file from the same repository. If already calculated add the projected latent vectors and/or even better the `attr_dirs.pkl` file."
      ],
      "metadata": {
        "id": "nKPNoW9r-Qr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Path definitions { display-mode: \"form\" }\n",
        "\n",
        "PROJECT_DIR = \"gen_faces\" #@param {type:\"string\"}\n",
        "PROJECT_PATH = f\"/content/drive/MyDrive/{PROJECT_DIR}\"\n",
        "SAVE_IMAGE_IN_DRIVE = True #@param {type:\"boolean\"}\n",
        "SAVE_IMAGE_PATH = f\"{PROJECT_PATH}/generated_images/\" if SAVE_IMAGE_IN_DRIVE else \"/content\"\n",
        "\n",
        "image_directory = f\"{PROJECT_PATH}/images\"\n",
        "vec_directory = f\"{PROJECT_PATH}/l_vecs\"\n",
        "\n",
        "import pathlib\n",
        "for path in [PROJECT_PATH, image_directory, vec_directory]:\n",
        "  pathlib.Path(path).mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "BeWa2RyqZKS0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "aFgHVjgyMRgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Stylegan for face generation"
      ],
      "metadata": {
        "id": "LynMi3t1LIDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja\n",
        "sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")\n",
        "!wget \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU2cMsA2rq1R",
        "outputId": "a093893c-0975-4da8-9f30-d6353e936b36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 7.92 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n",
            "--2023-08-28 08:40:47--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 52.84.18.96, 52.84.18.33, 52.84.18.74, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|52.84.18.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381624121 (364M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq.pkl’\n",
            "\n",
            "ffhq.pkl            100%[===================>] 363.94M  30.4MB/s    in 13s     \n",
            "\n",
            "2023-08-28 08:41:01 (28.7 MB/s) - ‘ffhq.pkl’ saved [381624121/381624121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Images onto Facespace"
      ],
      "metadata": {
        "id": "RJWlPSbfXbzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we use the stylegan projector tool to get the corresponding latent vector in w-space for each image used in the [omi-dataset](https://github.com/jcpeterson/omi). This can take up to 2 days so be sure to save the results in the corresponding directory. Only images where no corresponing latent vector is found will be projected."
      ],
      "metadata": {
        "id": "Z8foeErBd48k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "NETWORK = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "\n",
        "def remove_extension(file):\n",
        "  if file.lower().endswith('.jpg'):\n",
        "    return os.path.splitext(file)[0]\n",
        "\n",
        "def get_imagenames():\n",
        "  original = [remove_extension(f) for f in os.listdir(image_directory) if os.path.isfile(os.path.join(image_directory, f))]\n",
        "  done =  os.listdir(vec_directory)\n",
        "  res = [f for f in original if f not in done]\n",
        "  print(f\"original: {len(original)}, done: {len(done)}, left: {len(res)}\")\n",
        "  return res\n",
        "\n",
        "def gen_cmd(imagename):\n",
        "  return  f\"python stylegan2-ada-pytorch/projector.py\\\n",
        "        --save-video 0 --num-steps 1000\\\n",
        "        --outdir={vec_directory}/{imagename}\\\n",
        "        --target={image_directory}/{imagename}.jpg\\\n",
        "        --network={NETWORK}\"\n",
        "\n",
        "def project_images():\n",
        "  for imagename in tqdm(get_imagenames()):\n",
        "    !{gen_cmd(imagename)}\n"
      ],
      "metadata": {
        "id": "XQMWazm7X6kt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate attribute directions"
      ],
      "metadata": {
        "id": "N0WQO-StMObR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load the ratings from the [omi-dataset](https://github.com/jcpeterson/omi) and match them to the corresponding latent vecors. Then we do linear regression on each attribute to get the linear combination which characterizes it most. We call these combinations attribute directions."
      ],
      "metadata": {
        "id": "NLcQsvtTi9Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_w_space_vecs():\n",
        "  return {int(d):np.load(f\"{vec_directory}/{d}/projected_w.npz\")['w'] for d in tqdm(os.listdir(vec_directory))}\n",
        "\n",
        "def load_ratings():\n",
        "  ratings = pd.read_csv(f\"{PROJECT_PATH}/attribute_means.csv\")\n",
        "  ratings.set_index('stimulus',inplace=True)\n",
        "  return ratings\n",
        "\n",
        "def prepare_vecs_for_regression(vecs):\n",
        "  return np.array([value.flatten() for _, value in sorted(vecs.items())])\n",
        "\n",
        "def find_attr_dir(specific_ratings_):\n",
        "  X, y = prepare_vecs_for_regression(load_w_space_vecs()), specific_ratings_.to_numpy()\n",
        "  clf = RidgeCV().fit(X, y)\n",
        "  return clf.coef_.reshape(X[1].shape)\n",
        "\n",
        "def find_all_attr_dirs():\n",
        "  ratings = load_ratings()\n",
        "  return {ratingname : find_attr_dir(ratings[ratingname]) for ratingname in ratings.columns}\n",
        "\n",
        "def load_attr_dirs(path):\n",
        "  with open(path, \"rb\") as f:\n",
        "    return pickle.load(f)\n",
        "\n",
        "def save_attr_dirs(path):\n",
        "  with open(path, \"wb\") as f:\n",
        "      pickle.dump(attr_dirs, f)\n",
        "\n",
        "def get_attr_dirs():\n",
        "  path = f\"{PROJECT_PATH}/attr_dirs.pkl\"\n",
        "  if os.path.exists(path):\n",
        "    return load_attr_dirs(path)\n",
        "  project_images()\n",
        "  attr_dirs = find_all_attr_dirs()\n",
        "  save_attr_dirs(path)\n",
        "  return attr_dirs"
      ],
      "metadata": {
        "id": "XdMSoEKhZrhz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_V2JhqS6Zc8M"
      },
      "outputs": [],
      "source": [
        "r_attr_dirs = get_attr_dirs()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def normalize_dir(dir):\n",
        "  return dir / np.linalg.norm(dir)\n",
        "attr_dirs = {attr:normalize_dir(val) for attr, val in r_attr_dirs.items()}"
      ],
      "metadata": {
        "id": "oz9STYaNUB4l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## random latent vector generation\n",
        "\n",
        "\n",
        "For reference on mapping parameters see [here](https://github.com/NVlabs/stylegan/tree/1e0d5c781384ef12b50ef20a62fee5d78b38e88f#using-pre-trained-networks)."
      ],
      "metadata": {
        "id": "5Bb_mf7VvT4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be working with vectors within the W+ space. To get such a vector we first generate a random z vector and then map it to W+."
      ],
      "metadata": {
        "id": "oI34Rn7aFaoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "with open('ffhq.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\n",
        "\n",
        "def get_random_Z_space_vec():\n",
        "  return torch.randn([1, G.z_dim]).cuda()\n",
        "\n",
        "def get_random_W_space_vec():\n",
        "  z, c = get_random_Z_space_vec(), None\n",
        "  return G.mapping(z , c, truncation_psi=0.5, truncation_cutoff=8)\n"
      ],
      "metadata": {
        "id": "-uGeGoohvF4F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_blank_face():\n",
        "  return get_random_W_space_vec().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "I4FEKMum92Px"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure out mean and standard deviation"
      ],
      "metadata": {
        "id": "r-Uf9IKrrOIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We empirically calculate the mean and the covarianve matrix. Note that our sample is chosen based on a normal distribution over Z space wich is then mapped onto W. So the sample faces are not really representative of real life. A more curated sample would probably be better."
      ],
      "metadata": {
        "id": "r3EEkbO1Z3kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CALC_SIZE = 5000\n",
        "\n",
        "def empiric_mean():\n",
        "  return np.mean([get_blank_face() for _ in range(CALC_SIZE)], axis = 0)\n",
        "\n",
        "def empiric_cov():\n",
        "  return np.cov([get_blank_face().flatten() for _ in range(CALC_SIZE)], rowvar=False)"
      ],
      "metadata": {
        "id": "RgY3lzn5mPfe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oa_mean = empiric_mean()\n",
        "cov_mtrx = empiric_cov()"
      ],
      "metadata": {
        "id": "P9j5mmhK3ebm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ffa12b-75de-48ee-b409-3e9a6f3c69fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the mean face $\\mu_F\\in \\mathbb{R}^d$ we can calculate the mean along some direction $\\mu_d\\in\\mathbb{R}$. To do that we use the scalar projection since the direction is normalized.\n",
        "$$\n",
        "\\mu_d = \\langle \\mu_F , d \\rangle\n",
        "$$"
      ],
      "metadata": {
        "id": "qyKUAbpSaWBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_dot(a,b):\n",
        "  return np.dot(a.flatten(), b.flatten())\n",
        "\n",
        "def mean_of_dir(dir):\n",
        "  return flat_dot(oa_mean, normalize_dir(dir))"
      ],
      "metadata": {
        "id": "Vx9fQ6yLA2kI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then calculate the standard deviation $\\sigma_d$ along some direction $d$ by\n",
        "$$\n",
        "\\sigma_d = \\sqrt{ d^t\\ \\Sigma\\ d}\n",
        "$$"
      ],
      "metadata": {
        "id": "6LpPX0TdbSVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def std_of_dir(dir):\n",
        "  fdir = normalize_dir(dir.flatten())\n",
        "  return np.sqrt(np.matmul(np.matmul(fdir.transpose(), cov_mtrx), fdir))"
      ],
      "metadata": {
        "id": "LEiejIV8Fe-y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Face manipulation\n",
        "\n",
        "Our Goal is that if for any direction $d$ we can generate a face that is average in this dimension. We call this face the base face. In a further step we want to change the face by multiples of standard deviations $\\sigma_d$ in $d$. So assuming we have multiples -1,0.1 the scalar projection of the generated faces onto $d$ will be $-\\sigma_d, 0, \\sigma_d$ respectively."
      ],
      "metadata": {
        "id": "sTtIT1r-e6AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When analysing a face along some direction we want to first standardize the face, such that the base face is average along the direction.\n",
        "$$\n",
        "f - ⟨f,d⟩ d  + μ_d d\n",
        "=\n",
        "f - (⟨f,d⟩ - μ_d) d\n",
        "$$"
      ],
      "metadata": {
        "id": "Wrta2XuJF8pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_face(attr_dir):\n",
        "  base = get_blank_face()\n",
        "  return standardize_face(base, attr_dir, mean_of_dir(attr_dir))\n",
        "\n",
        "def standardize_face(face, dir, mean):\n",
        "  sdir = normalize_dir(dir)\n",
        "  return face - sdir * (flat_dot(sdir,face)-mean)\n"
      ],
      "metadata": {
        "id": "9paeaEs8d_9H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have meaningful step distances we want to change the magnitude of the direction $d$ to its standard deviation $σ_d$."
      ],
      "metadata": {
        "id": "OyeXLru3e9k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_dir(dir):\n",
        "  sdir = normalize_dir(dir)\n",
        "  return std_of_dir(sdir) * sdir\n"
      ],
      "metadata": {
        "id": "zHCiWuqie7C4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When editing a face we might want to edit along one attribute vector $A$ without moving along another attribute $S$. To do this we project $A$ onto $S^\\perp$(the orthogonal complement of $S$). This\n",
        "results in the following directiuon:\n",
        "$$\n",
        "A - \\langle A , S \\rangle S\n",
        "$$\n"
      ],
      "metadata": {
        "id": "vqhyoEGqOxyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orth_proj(a,b):\n",
        "  assert np.isclose(np.linalg.norm(b), 1)\n",
        "  return b * flat_dot(a,b)\n",
        "\n",
        "def raw_stab_dir(a,s):\n",
        "  return a - orth_proj(a,s)\n"
      ],
      "metadata": {
        "id": "oFYoUFYI_aG7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Face generation"
      ],
      "metadata": {
        "id": "2qUyFW-o4hxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to generate multiple faces, as described above. We generate them using the synthesis network. since the contrast is weird we normalize it in a very simple way."
      ],
      "metadata": {
        "id": "9QsfaFzWDWzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def synthesize_face(l_vec):\n",
        "  w = torch.from_numpy(l_vec).cuda()\n",
        "  return  G.synthesis(w, noise_mode='const', force_fp32=True)\n",
        "\n",
        "def normalize_face_contrast(image):\n",
        "  min_val = float(torch.min(image))\n",
        "  max_val = float(torch.max(image))\n",
        "  amp = max_val - min_val\n",
        "  return (image - min_val)/amp\n",
        "\n",
        "def create_face(l_vec):\n",
        "  return normalize_face_contrast(synthesize_face(l_vec))\n",
        "\n",
        "def get_multiple_faces(base, attr_dir, mults):\n",
        "  return {mult:create_face(base + attr_dir * mult) for mult in mults}\n"
      ],
      "metadata": {
        "id": "GXhF0r_6rNLL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "from os import makedirs\n",
        "\n",
        "def save_facematrix(face_matrix):\n",
        "  for i, faces in enumerate(face_matrix):\n",
        "    makedirs(f'{SAVE_IMAGE_PATH}{i}', exist_ok=True)\n",
        "    for val, face in faces.items():\n",
        "      save_image(face, f'{SAVE_IMAGE_PATH}{i}/{val}.png')"
      ],
      "metadata": {
        "id": "C3Cpsy0twLOF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "iRYa0nCvNNPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main Code { display-mode: \"form\" }\n",
        "\n",
        "AMOUNT_OF_FACES = 10 #@param {type:\"integer\", min: 1, max:100}\n",
        "ATTRIBUTE_TO_CHANGE = 'outdoors' #@param ['trustworthy', 'attractive', 'dominant', 'smart', 'age', 'gender', 'weight', 'typical', 'happy', 'familiar', 'outgoing', 'memorable', 'well-groomed', 'long-haired', 'smug', 'dorky', 'skin-color', 'hair-color', 'alert', 'cute', 'privileged', 'liberal', 'asian', 'middle-eastern', 'hispanic', 'islander', 'native', 'black', 'white', 'looks-like-you', 'gay', 'electable', 'godly', 'outdoors']\n",
        "SHOULD_STABILIZE = False #@param {type:\"boolean\"}\n",
        "STABILIZE_BY = 'age'#@param ['trustworthy', 'attractive', 'dominant', 'smart', 'age', 'gender', 'weight', 'typical', 'happy', 'familiar', 'outgoing', 'memorable', 'well-groomed', 'long-haired', 'smug', 'dorky', 'skin-color', 'hair-color', 'alert', 'cute', 'privileged', 'liberal', 'asian', 'middle-eastern', 'hispanic', 'islander', 'native', 'black', 'white', 'looks-like-you', 'gay', 'electable', 'godly', 'outdoors']\n",
        "CHANGE_FACTORS = \"-8,-4,0,4,8\" #@param [[-10, -5, -2,  0, 2 , 5, 10], [-8, -4,  0, 4 , 8]] {allow-input: true}\n",
        "CHANGE_FACTORS = [int(i) for i in CHANGE_FACTORS.split(\",\")]\n",
        "\n",
        "dir = attr_dirs[ATTRIBUTE_TO_CHANGE]\n",
        "if SHOULD_STABILIZE:\n",
        "  dir = raw_stab_dir(dir ,attr_dirs[STABILIZE_BY])\n",
        "sdir = standardize_dir(dir)\n",
        "base_faces = [get_base_face(sdir) for _ in range(AMOUNT_OF_FACES)]\n",
        "face_matrix = [get_multiple_faces(base_face, sdir, CHANGE_FACTORS ) for base_face in base_faces]\n",
        "save_facematrix(face_matrix)"
      ],
      "metadata": {
        "id": "5omZkWoKw7iT"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}